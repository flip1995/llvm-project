; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: asserts
; RUN: %cheri_purecap_opt -cheri-bound-allocas %s -o - -S -cheri-stack-bounds=if-needed \
; RUN:    -cheri-stack-bounds-single-intrinsic-threshold=10 -debug-only=cheri-bound-allocas 2>%t.dbg | FileCheck %s
; RUN: %cheri_purecap_llc -cheri-stack-bounds=if-needed -O2 -cheri-stack-bounds-single-intrinsic-threshold=10 -o - %s | %cheri_FileCheck %s -check-prefix ASM
; RUN: FileCheck %s -check-prefix DBG -input-file=%t.dbg

target datalayout = "Eme-pf200:128:128:128:64-A200-P200-G200"

declare void @foo(i32 addrspace(200)*) addrspace(200)

; Check that we don't attempt to insert stack bounds intrinisics before the PHI at the start of a basic block:
define void @test_phi(i1 %cond) addrspace(200) nounwind {
; CHECK-LABEL: @test_phi(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ALLOCA1:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA2:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA3:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    br i1 [[COND:%.*]], label [[BLOCK1:%.*]], label [[BLOCK2:%.*]]
; CHECK:       block1:
; CHECK-NEXT:    store i32 1, i32 addrspace(200)* [[ALLOCA1]], align 4
; CHECK-NEXT:    store i32 2, i32 addrspace(200)* [[ALLOCA2]], align 4
; CHECK-NEXT:    store i32 3, i32 addrspace(200)* [[ALLOCA3]], align 4
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA2]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK:%.*]]
; CHECK:       block2:
; CHECK-NEXT:    store i32 4, i32 addrspace(200)* [[ALLOCA1]], align 4
; CHECK-NEXT:    store i32 5, i32 addrspace(200)* [[ALLOCA2]], align 4
; CHECK-NEXT:    store i32 6, i32 addrspace(200)* [[ALLOCA3]], align 4
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA1]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; CHECK-NEXT:    [[TMP6:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA3]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP7:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP6]], i64 4)
; CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 addrspace(200)* [[TMP7]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK]]
; CHECK:       phi_block:
; CHECK-NEXT:    [[VAL1:%.*]] = phi i32 addrspace(200)* [ null, [[BLOCK1]] ], [ [[TMP5]], [[BLOCK2]] ]
; CHECK-NEXT:    [[VAL2:%.*]] = phi i32 addrspace(200)* [ [[TMP2]], [[BLOCK1]] ], [ [[TMP8]], [[BLOCK2]] ]
; CHECK-NEXT:    call void @foo(i32 addrspace(200)* [[VAL1]])
; CHECK-NEXT:    call void @foo(i32 addrspace(200)* [[VAL2]])
; CHECK-NEXT:    ret void
;
; ASM-LABEL: test_phi:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    cincoffset $c11, $c11, -[[STACKFRAME_SIZE:64|128]]
; ASM-NEXT:    csc $c19, $zero, [[#CAP_SIZE * 3]]($c11)
; ASM-NEXT:    csc $c18, $zero, [[#CAP_SIZE * 2]]($c11)
; ASM-NEXT:    csc $c17, $zero, [[#CAP_SIZE * 1]]($c11)
; ASM-NEXT:    lui $1, %hi(%neg(%captab_rel(test_phi)))
; ASM-NEXT:    daddiu $1, $1, %lo(%neg(%captab_rel(test_phi)))
; ASM-NEXT:    sll $2, $4, 0
; ASM-NEXT:    andi $2, $2, 1
; ASM-NEXT:    beqz $2, .LBB0_2
; ASM-NEXT:    cincoffset $c19, $c12, $1
; ASM-NEXT:  # %bb.1: # %block1
; ASM-NEXT:    addiu $1, $zero, 1
; ASM-NEXT:    csw $1, $zero, {{12|28}}($c11)
; ASM-NEXT:    addiu $1, $zero, 2
; ASM-NEXT:    csw $1, $zero, {{8|24}}($c11)
; ASM-NEXT:    addiu $1, $zero, 3
; ASM-NEXT:    csw $1, $zero, {{4|20}}($c11)
; ASM-NEXT:    cincoffset $c18, $c11, {{8|24}}
; ASM-NEXT:    csetbounds $c18, $c18, 4
; ASM-NEXT:    b .LBB0_3
; ASM-NEXT:    cgetnull $c3
; ASM-NEXT:  .LBB0_2: # %block2
; ASM-NEXT:    addiu $1, $zero, 4
; ASM-NEXT:    csw $1, $zero, {{12|28}}($c11)
; ASM-NEXT:    addiu $1, $zero, 5
; ASM-NEXT:    csw $1, $zero, {{8|24}}($c11)
; ASM-NEXT:    addiu $1, $zero, 6
; ASM-NEXT:    csw $1, $zero, {{4|20}}($c11)
; ASM-NEXT:    cincoffset $c3, $c11, {{12|28}}
; ASM-NEXT:    csetbounds $c3, $c3, 4
; ASM-NEXT:    cincoffset $c18, $c11, {{4|20}}
; ASM-NEXT:    csetbounds $c18, $c18, 4
; ASM-NEXT:  .LBB0_3: # %phi_block
; ASM-NEXT:    clcbi $c12, %capcall20(foo)($c19)
; ASM-NEXT:    cjalr $c12, $c17
; ASM-NEXT:    nop
; ASM-NEXT:    clcbi $c12, %capcall20(foo)($c19)
; ASM-NEXT:    cjalr $c12, $c17
; ASM-NEXT:    cmove $c3, $c18
; ASM-NEXT:    clc $c17, $zero, [[#CAP_SIZE * 1]]($c11)
; ASM-NEXT:    clc $c18, $zero, [[#CAP_SIZE * 2]]($c11)
; ASM-NEXT:    clc $c19, $zero, [[#CAP_SIZE * 3]]($c11)
; ASM-NEXT:    cjr $c17
; ASM-NEXT:    cincoffset $c11, $c11, [[STACKFRAME_SIZE]]
entry:
  %alloca1 = alloca i32, align 4, addrspace(200)
  %alloca2 = alloca i32, align 4, addrspace(200)
  %alloca3 = alloca i32, align 4, addrspace(200)
  br i1 %cond, label %block1, label %block2

block1:
  store i32 1, i32 addrspace(200)* %alloca1, align 4
  store i32 2, i32 addrspace(200)* %alloca2, align 4
  store i32 3, i32 addrspace(200)* %alloca3, align 4
  br label %phi_block

block2:
  store i32 4, i32 addrspace(200)* %alloca1, align 4
  store i32 5, i32 addrspace(200)* %alloca2, align 4
  store i32 6, i32 addrspace(200)* %alloca3, align 4
  br label %phi_block

phi_block:
  %val1 = phi i32 addrspace(200)* [ null, %block1 ], [ %alloca1, %block2 ]
  %val2 = phi i32 addrspace(200)* [ %alloca2, %block1 ], [ %alloca3, %block2 ]
  call void @foo(i32 addrspace(200)* %val1)
  call void @foo(i32 addrspace(200)* %val2)
  ret void
}

; Check that we don't place all bounded allocas in the entry block, instead only do it in the predecessor
define void @test_only_created_in_predecessor_block(i1 %cond) addrspace(200) nounwind {
; CHECK-LABEL: @test_only_created_in_predecessor_block(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ALLOCA1:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    [[ALLOCA2:%.*]] = alloca i32, align 4, addrspace(200)
; CHECK-NEXT:    br i1 [[COND:%.*]], label [[BLOCK1:%.*]], label [[BLOCK2:%.*]]
; CHECK:       block1:
; CHECK-NEXT:    store i32 1, i32 addrspace(200)* [[ALLOCA1]], align 4
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA1]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP1:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP0]], i64 4)
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i8 addrspace(200)* [[TMP1]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK:%.*]]
; CHECK:       block2:
; CHECK-NEXT:    store i32 5, i32 addrspace(200)* [[ALLOCA2]], align 4
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i32 addrspace(200)* [[ALLOCA2]] to i8 addrspace(200)*
; CHECK-NEXT:    [[TMP4:%.*]] = call i8 addrspace(200)* @llvm.cheri.bounded.stack.cap.i64(i8 addrspace(200)* [[TMP3]], i64 4)
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 addrspace(200)* [[TMP4]] to i32 addrspace(200)*
; CHECK-NEXT:    br label [[PHI_BLOCK]]
; CHECK:       phi_block:
; CHECK-NEXT:    [[VAL1:%.*]] = phi i32 addrspace(200)* [ [[TMP2]], [[BLOCK1]] ], [ [[TMP5]], [[BLOCK2]] ]
; CHECK-NEXT:    call void @foo(i32 addrspace(200)* [[VAL1]])
; CHECK-NEXT:    ret void
;
; ASM-LABEL: test_only_created_in_predecessor_block:
; ASM:       # %bb.0: # %entry
; ASM-NEXT:    cincoffset $c11, $c11, -[[STACKFRAME_SIZE:32|64]]
; ASM-NEXT:    csc $c17, $zero, [[#CAP_SIZE]]($c11)
; ASM-NEXT:    lui $1, %hi(%neg(%captab_rel(test_only_created_in_predecessor_block)))
; ASM-NEXT:    daddiu $1, $1, %lo(%neg(%captab_rel(test_only_created_in_predecessor_block)))
; ASM-NEXT:    sll $2, $4, 0
; ASM-NEXT:    andi $2, $2, 1
; ASM-NEXT:    beqz $2, .LBB1_2
; ASM-NEXT:    cincoffset $c1, $c12, $1
; ASM-NEXT:  # %bb.1: # %block1
; ASM-NEXT:    addiu $1, $zero, 1
; ASM-NEXT:    csw $1, $zero, {{12|28}}($c11)
; ASM-NEXT:    cincoffset $c3, $c11, {{12|28}}
; ASM-NEXT:    b .LBB1_3
; ASM-NEXT:    csetbounds $c3, $c3, 4
; ASM-NEXT:  .LBB1_2: # %block2
; ASM-NEXT:    addiu $1, $zero, 5
; ASM-NEXT:    csw $1, $zero, {{8|24}}($c11)
; ASM-NEXT:    cincoffset $c3, $c11, {{8|24}}
; ASM-NEXT:    csetbounds $c3, $c3, 4
; ASM-NEXT:  .LBB1_3: # %phi_block
; ASM-NEXT:    clcbi $c12, %capcall20(foo)($c1)
; ASM-NEXT:    cjalr $c12, $c17
; ASM-NEXT:    nop
; ASM-NEXT:    clc $c17, $zero, [[#CAP_SIZE]]($c11)
; ASM-NEXT:    cjr $c17
; ASM-NEXT:    cincoffset $c11, $c11, [[STACKFRAME_SIZE]]
entry:
  %alloca1 = alloca i32, align 4, addrspace(200)
  %alloca2 = alloca i32, align 4, addrspace(200)
  br i1 %cond, label %block1, label %block2

block1:
  store i32 1, i32 addrspace(200)* %alloca1, align 4
  br label %phi_block

block2:
  store i32 5, i32 addrspace(200)* %alloca2, align 4
  br label %phi_block

phi_block:
  %val1 = phi i32 addrspace(200)* [ %alloca1, %block1 ], [ %alloca2, %block2 ]
  call void @foo(i32 addrspace(200)* %val1)
  ret void
}


; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(i32 addrspace(200)* %val1)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca1 = alloca i32, align 4, addrspace(200)
; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(i32 addrspace(200)* %val2)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca2 = alloca i32, align 4, addrspace(200)
; DBG: -Adding stack bounds since phi user needs bounds:   call void @foo(i32 addrspace(200)* %val2)
; DBG: test_phi: 1 of 3 users need bounds for   %alloca3 = alloca i32, align 4, addrspace(200)
